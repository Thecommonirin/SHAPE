# SHAPE: Self-Improved Holistic Alignment for Preference Enhancement

<div align="center">

<img src="assets/logo.png" alt="SHAPE Logo" width="200"/>

**è‡ªç›‘ç£å¹»è§‰å¯¹é½ä¸åå¥½å¢å¼ºæ¡†æ¶**

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)

</div>

---

## ğŸ“– Introduction

**SHAPE** is an innovative preference alignment framework for Large Vision-Language Models (LVLMs). It focuses on mitigating hallucinations and enhancing model reliability through **Direct Preference Optimization (DPO)** without relying on expensive human annotations.

### âœ¨ Key Features

- **Self-Supervised Alignment**: Transforms existing supervised image-text pairs into preference tuplets via visual augmentation and summarization.
- **DPO Training**: Efficient preference learning without complex Reinforcement Learning (RL) pipelines.
- **Reward Model Guidance**: Utilizes a lightweight model (e.g., Tiny-LLaVA) as a reward signal provider.
- **Fused Inference**: Token-wise fusion of the base model and reward model outputs for robust generation.
- **Hallucination Mitigation**: Significantly reduces hallucinations on benchmarks like POPE, OCR-VQA, and TextVQA.
- **Modular Design**: Clean, extensible code structure for easy customization.

### ğŸ” Comparison with Other Methods

| Feature | SHAPE (Ours) | TITA | SeVa |
| :--- | :---: | :---: | :---: |
| **Data Generation** | **Reward-Guided / Summarization** | Iterative Training | Image Augmentation |
| **Training Method** | **DPO** | Hybrid PPO/DPO | DPO |
| **Inference** | **Fused / Holistic** | Single Model | Single Model |
| **Architecture** | **Modular** | Monolithic | Basic |

---

##  ğŸ“‚ Project Structure

```
shape/
â”œâ”€â”€ configs/                    # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ config.py              # é…ç½®ç®¡ç†
â”‚   â””â”€â”€ deepspeed/             # DeepSpeed é…ç½®
â”‚       â”œâ”€â”€ zero2.json
â”‚       â”œâ”€â”€ zero3.json
â”‚       â””â”€â”€ zero3_offload.json
â”œâ”€â”€ shape/                      # æ ¸å¿ƒæ¨¡å—
â”‚   â”œâ”€â”€ core/                  # æ ¸å¿ƒå·¥å…·
â”‚   â”‚   â”œâ”€â”€ logger.py          # æ—¥å¿—ç³»ç»Ÿ
â”‚   â”‚   â””â”€â”€ utils.py           # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ training/              # è®­ç»ƒæ¨¡å—
â”‚   â”‚   â”œâ”€â”€ preference_trainer.py  # åå¥½å¯¹é½è®­ç»ƒå™¨
â”‚   â”‚   â”œâ”€â”€ llava_dpo_trainer.py   # DPO è®­ç»ƒå™¨
â”‚   â”‚   â””â”€â”€ base_dpo_trainer.py    # åŸºç¡€è®­ç»ƒå™¨
â”‚   â””â”€â”€ evaluation/            # è¯„ä¼°æ¨¡å—
â”‚       â”œâ”€â”€ hallucination_metrics.py  # å¹»è§‰æŒ‡æ ‡
â”‚       â””â”€â”€ benchmarks.py      # åŸºå‡†æµ‹è¯•
â”œâ”€â”€ src/                        # LLaVA æºç 
â”‚   â””â”€â”€ llava/                 # LLaVA æ¨¡å‹å®ç°
â”œâ”€â”€ datasets/                   # è®­ç»ƒæ•°æ®
â”‚   â”œâ”€â”€ ocrvqa_answer_file_8k_dpo.jsonl
â”‚   â””â”€â”€ textvqa_answer_file_8k_dpo.jsonl
â”œâ”€â”€ train_preference_alignment.py  # è®­ç»ƒä¸»è„šæœ¬
â”œâ”€â”€ fused_inference.py         # èåˆæ¨ç†è„šæœ¬
â”œâ”€â”€ train.sh                   # è®­ç»ƒå¯åŠ¨è„šæœ¬
â”œâ”€â”€ pyproject.toml             # é¡¹ç›®é…ç½®
â””â”€â”€ README.md                  # æœ¬æ–‡æ¡£
```

---

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n shape python=3.10 -y
conda activate shape

# å®‰è£…ä¾èµ–
pip install torch==2.0.1 torchvision==0.15.2
pip install -e .

# å®‰è£…è®­ç»ƒä¾èµ–
pip install -e ".[training]"
```

### Data Preparation

#### 1. Preference Data

We provide pre-generated DPO training data in the datasets/ folder:

```
datasets/
â”œâ”€â”€ ocrvqa_answer_file_8k_dpo.jsonl      # OCR-VQA åå¥½å¯¹
â””â”€â”€ textvqa_answer_file_8k_dpo.jsonl     # TextVQA åå¥½å¯¹
```

Data Format:
```json
{
  "chosen": "æ­£ç¡®æˆ–æ›´å¥½çš„å›ç­”",
  "reject": "é”™è¯¯æˆ–è¾ƒå·®çš„å›ç­”",
  "question": "é—®é¢˜æ–‡æœ¬",
  "image_id": "å›¾åƒæ–‡ä»¶å"
}
```

#### 2. Image Data

Please download the corresponding image datasets and place them in the data/ directory:

```bash
# åˆ›å»ºæ•°æ®ç›®å½•
mkdir -p data/textvqa data/ocrvqa

# ä¸‹è½½ TextVQA å›¾åƒ
wget https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip
unzip train_val_images.zip -d data/textvqa/

# ä¸‹è½½ OCR-VQA å›¾åƒï¼ˆå‚è€ƒå®˜æ–¹è¯´æ˜ï¼‰
# https://ocr-vqa.github.io/
```

### Model Preparation

Clone the base model and reward model weights:

```bash
# åŸºç¡€æ¨¡å‹
git clone https://huggingface.co/liuhaotian/llava-v1.5-7b

# å¥–åŠ±æ¨¡å‹
git clone https://huggingface.co/bczhou/tiny-llava-v1-hf
```

---

##  Training

### Quick Run
You can start training using the provided shell script:

```bash
bash train.sh
```

### Custom Training Command

ç¼–è¾‘ `train.sh` æˆ–ç›´æ¥è¿è¡Œï¼š

```bash
python train_preference_alignment.py \
    --model_name_or_path bczhou/tiny-llava-v1-hf \
    --vision_tower openai/clip-vit-large-patch14-336 \
    --ocr_data_path ./datasets/ocrvqa_answer_file_8k_dpo.jsonl \
    --ocr_image_path data/ocrvqa/images/ \
    --textvqa_data_path ./datasets/textvqa_answer_file_8k_dpo.jsonl \
    --textvqa_image_path data/textvqa/train_images \
    --output_dir checkpoints/shape_model \
    --beta 0.1 \
    --learning_rate 2e-6 \
    --num_train_epochs 1 \
    --per_device_train_batch_size 8 \
    --gradient_checkpointing True \
    --bf16 True \
    --deepspeed configs/deepspeed/zero3_offload.json
```

### Key Arguments:

| Argument | Description | Default |
|-----|------|--------|
| `--beta` | DPO loss çš„æ¸©åº¦å‚æ•° | 0.1 |
| `--learning_rate` | å­¦ä¹ ç‡ | 2e-6 |
| `--num_train_epochs` | è®­ç»ƒè½®æ•° | 1 |
| `--per_device_train_batch_size` | æ¯ä¸ªè®¾å¤‡çš„æ‰¹æ¬¡å¤§å° | 8 |
| `--gradient_checkpointing` | æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰ | True |
| `--bf16` | ä½¿ç”¨ BF16 æ··åˆç²¾åº¦ | True |

---

## ğŸ”® Inference

### Fused Inference

Perform inference by fusing logits from the Base Model and the Reward Model:

```bash
python fused_inference.py \
    --base-model path/to/llava-1.5-7b \
    --reward-model path/to/tiny-llava \
    --weight-base 0.7 \
    --weight-reward 0.3 \
    --dataset MM-Vet/mm-vet \
    --output results/fused_inference.json
```


### Other Benchmarks

For other comprehensive benchmarks (e.g., MME, MMBench), please refer to the official LLaVA Evaluation Docs.



## ğŸ™ Acknowledgements

This project is built upon the following excellent works:

- [**LLaVA**](https://github.com/haotian-liu/LLaVA) - å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹
- [**DPO**](https://arxiv.org/abs/2305.18290) - ç›´æ¥åå¥½ä¼˜åŒ–æ–¹æ³•
- [**SeVa**](https://github.com/Kevinz-code/SeVa) - è‡ªç›‘ç£è§†è§‰åå¥½å¯¹é½
- [**HA-DPO**](https://github.com/opendatalab/HA-DPO/) - å¹»è§‰æ„ŸçŸ¥çš„ DPO

---

## ğŸ“ Citation

å¦‚æœæ‚¨ä½¿ç”¨æœ¬é¡¹ç›®ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@waiting
```

---

##  è®¸å¯è¯

This project is licensed under the Apache License 2.0.

---

<div align="center">

â­ If this project helps you, please give us a Star! â­

Made with â¤ï¸ by SHAPE Team

</div>
